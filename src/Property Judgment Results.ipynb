{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6cb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e54bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1940a7fb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c78185",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d46f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = defaultdict(list)\n",
    "with open(f'../data/experimental splits/train_1ns.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(f)\n",
    "    for line in reader:\n",
    "        sentence,label,concept,category,feature,idx = line\n",
    "        train_data[concept].append(float(label))\n",
    "        train_data[f'{concept}_{feature.split(\" \")[0].strip()}']\n",
    "#         data.append([idx, sentence, int(label), concept, category, feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681a330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_bias = {k: int((torch.tensor(v).mean() > 0.5).item()) for k,v in train_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1095c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split):\n",
    "    data = []\n",
    "    with open(f'../data/experimental splits/{split}_1ns.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(f)\n",
    "        for line in tqdm(reader):\n",
    "            sentence,label,concept,category,feature,idx = line\n",
    "            data.append([idx, sentence, int(label), concept, category, feature])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368ecc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyJudge:\n",
    "    def __init__(self, model_path, device='cpu'):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.device = device\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def tokenize(self, batch):\n",
    "        return self.tokenizer(batch, padding=True, return_tensors='pt').to(self.device)\n",
    "        \n",
    "    def infer(self, batch):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**batch).logits.detach()\n",
    "            logprob = logits - logits.logsumexp(1).unsqueeze(1)\n",
    "\n",
    "            predicted_labels = logprob.argmax(1).tolist()\n",
    "        \n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e3380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'axxl-property'\n",
    "PATH = f'../../induction/checkpoints/finetuned_models/{MODEL}'\n",
    "\n",
    "propjudge = PropertyJudge(PATH, 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330bed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6788it [00:00, 689318.83it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = load_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df430e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth2 = []\n",
    "cb_predicted = []\n",
    "bg_predicted = []\n",
    "catches = 0\n",
    "for entry in test_data:\n",
    "    idx, sentence, label, concept, category, feature = entry\n",
    "    truth2.append(label)\n",
    "    cb_predicted.append(concept_bias[concept])\n",
    "    try:\n",
    "        bg_predicted.append(concept_bias[f'{concept}_{feature.split(\" \")[0].strip()}'])\n",
    "    except:\n",
    "        catches+=1\n",
    "        bg_predicted.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98d62ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 213/213 [00:15<00:00, 13.49it/s]\n"
     ]
    }
   ],
   "source": [
    "truth = []\n",
    "predicted = []\n",
    "model_results = []\n",
    "test_dl = DataLoader(test_data, batch_size = 32, num_workers = 16)\n",
    "\n",
    "for batch in tqdm(test_dl):\n",
    "    idx, sentences, labels, concept, category, feature = batch\n",
    "    labels = labels.tolist()\n",
    "    sentences = list(sentences)\n",
    "    encoded = propjudge.tokenize(sentences)\n",
    "    pred = propjudge.infer(encoded)\n",
    "    \n",
    "    truth.extend(labels)\n",
    "    predicted.extend(pred)\n",
    "    \n",
    "    model_results.extend(list(zip(idx, sentences, concept, category, feature, labels, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15685a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/results/property-judgment/{MODEL}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['model', 'idx', 'sentence', 'concept', 'category', 'feature', 'label', 'predicted'])\n",
    "    for result in model_results:\n",
    "        idx, sentences, concept, category, feature, labels, pred\n",
    "        writer.writerow([MODEL, idx, sentences, concept, category, feature, labels, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93b2fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78274311410905"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(truth, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e8d85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "\n",
    "def eval_with_paired_bootstrap(gold, sys1, sys2, num_samples=10000, sample_ratio=0.5):\n",
    "    \"\"\"Evaluate with paired boostrap\n",
    "    This compares two systems, performing a significance tests with\n",
    "    paired bootstrap resampling to compare the accuracy of the two systems.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gold\n",
    "      The correct labels\n",
    "    sys1\n",
    "      The output of system 1\n",
    "    sys2\n",
    "      The output of system 2\n",
    "    num_samples\n",
    "      The number of bootstrap samples to take\n",
    "    sample_ratio\n",
    "      The ratio of samples to take every time\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(gold) == len(sys1)\n",
    "    assert len(gold) == len(sys2)\n",
    "\n",
    "    gold = np.array(gold)\n",
    "    sys1 = np.array(sys1)\n",
    "    sys2 = np.array(sys2)\n",
    "\n",
    "    sys1_scores = []\n",
    "    sys2_scores = []\n",
    "    wins = [0, 0, 0]\n",
    "    n = len(gold)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Subsample the gold and system outputs\n",
    "        subset_idxs = rng.choice(n, int(n * sample_ratio), replace=True)\n",
    "#         sys1_score = (sys1[subset_idxs] == gold[subset_idxs]).mean()\n",
    "#         sys2_score = (sys2[subset_idxs] == gold[subset_idxs]).mean()\n",
    "        sys1_score = f1_score(gold[subset_idxs], sys1[subset_idxs])\n",
    "        sys2_score = f1_score(gold[subset_idxs], sys2[subset_idxs])\n",
    "\n",
    "        if sys1_score > sys2_score:\n",
    "            wins[0] += 1\n",
    "        elif sys1_score < sys2_score:\n",
    "            wins[1] += 1\n",
    "        else:\n",
    "            wins[2] += 1\n",
    "\n",
    "        sys1_scores.append(sys1_score)\n",
    "        sys2_scores.append(sys2_score)\n",
    "\n",
    "    # Print win stats\n",
    "    wins = [x / float(num_samples) for x in wins]\n",
    "    print(\"Win ratio: sys1=%.3f, sys2=%.3f, tie=%.3f\" % (wins[0], wins[1], wins[2]))\n",
    "    if wins[0] > wins[1]:\n",
    "        print(\"(sys1 is superior with p value p=%.3f)\\n\" % (1 - wins[0]))\n",
    "    elif wins[1] > wins[0]:\n",
    "        print(\"(sys2 is superior with p value p=%.3f)\\n\" % (1 - wins[1]))\n",
    "\n",
    "    # Print system stats\n",
    "    sys1_scores.sort()\n",
    "    sys2_scores.sort()\n",
    "    print(\n",
    "        \"sys1 mean=%.3f, median=%.3f, 95%% confidence interval=[%.3f, %.3f], sd=%.3f\"\n",
    "        % (\n",
    "            np.mean(sys1_scores),\n",
    "            np.median(sys1_scores),\n",
    "            sys1_scores[int(num_samples * 0.025)],\n",
    "            sys1_scores[int(num_samples * 0.975)],\n",
    "            np.mean(sys1_scores) - sys1_scores[int(num_samples * 0.025)]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"sys2 mean=%.3f, median=%.3f, 95%% confidence interval=[%.3f, %.3f], sd=%.3f\"\n",
    "        % (\n",
    "            np.mean(sys2_scores),\n",
    "            np.median(sys2_scores),\n",
    "            sys2_scores[int(num_samples * 0.025)],\n",
    "            sys2_scores[int(num_samples * 0.975)],\n",
    "            np.mean(sys2_scores) - sys2_scores[int(num_samples * 0.025)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82a1c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win ratio: sys1=1.000, sys2=0.000, tie=0.000\n",
      "(sys1 is superior with p value p=0.000)\n",
      "\n",
      "sys1 mean=0.790, median=0.791, 95% confidence interval=[0.776, 0.805], sd=0.015\n",
      "sys2 mean=0.652, median=0.652, 95% confidence interval=[0.633, 0.670], sd=0.019\n"
     ]
    }
   ],
   "source": [
    "eval_with_paired_bootstrap(truth, predicted, cb_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66c2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win ratio: sys1=1.000, sys2=0.000, tie=0.000\n",
      "(sys1 is superior with p value p=0.000)\n",
      "\n",
      "sys1 mean=0.791, median=0.791, 95% confidence interval=[0.775, 0.805], sd=0.015\n",
      "sys2 mean=0.667, median=0.667, 95% confidence interval=[0.652, 0.681], sd=0.015\n"
     ]
    }
   ],
   "source": [
    "eval_with_paired_bootstrap(truth, predicted, [1]*len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01912441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
