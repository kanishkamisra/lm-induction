%
%  all-biblatex.bib  2021-08-21  Mark Senn  http://bit.ly/marksenn
%
%  This is an example BibLaTeX data file.
%
%  I like to order the entries by author, and within author by date.
%  If, for example, I later added a second 1994 publication for Goossens,
%  I would give it the key, "goossens1994b".
%
%  From http://isbn.org/about_ISBN_standard
%      For more than thirty years, ISBNs were 10 digits long.
%      On January 1, 2007 the ISBN system switched to a 13-digit
%      format.  Now all ISBNs are 13-digits long.
%
%  The apa7-<number> references are from the _Publication Manual
%  of the American Psychological Association_, 7th edition,
%  pages 317--352.
%

@book{machery2009doing,
  title={Doing without concepts},
  author={Machery, Edouard},
  year={2009},
  publisher={Oxford University Press}
}

@inproceedings{derby-etal-2021-representation,
    title = "Representation and Pre-Activation of Lexical-Semantic Knowledge in Neural Language Models",
    author = "Derby, Steven  and
      Miller, Paul  and
      Devereux, Barry",
    booktitle = "{Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics}",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "{Association for Computational Linguistics}",
    url = "https://aclanthology.org/2021.cmcl-1.25",
    doi = "10.18653/v1/2021.cmcl-1.25",
    pages = "211--221",
    abstract = "In this paper, we perform a systematic analysis of how closely the intermediate layers from LSTM and trans former language models correspond to human semantic knowledge. Furthermore, in order to make more meaningful comparisons with theories of human language comprehension in psycholinguistics, we focus on two key stages where the meaning of a particular target word may arise: immediately before the word{'}s presentation to the model (comparable to forward inferencing), and immediately after the word token has been input into the network. Our results indicate that the transformer models are better at capturing semantic knowledge relating to lexical concepts, both during word prediction and when retention is required.",
}

@article{bhatia2020transformer,
  title={Transformer Networks of Human Concept Knowledge},
  author={Bhatia, Sudeep and Richie, Russell},
  journal={Psychological Review},
  year={2021},
  publisher={American Psychological Association}
}

@inproceedings{mccoy2020universal,
  title={{Universal linguistic inductive biases via meta-learning}},
  author={McCoy, R Thomas and Grant, Erin and Smolensky, Paul and Griffiths, Thomas L and Linzen, Tal},
  year={2020},
  booktitle={{Proceedings of the 42nd Annual Conference of the Cognitive Science
Society}}
}

@article{mccoy2020does,
  title={{Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks}},
  author={McCoy, R Thomas and Frank, Robert and Linzen, Tal},
  journal={{Transactions of the Association for Computational Linguistics}},
  volume={8},
  pages={125--140},
  year={2020},
  publisher={{MIT Press}}
}


@inproceedings{derby-etal-2019-feature2vec,
    title = "{F}eature2{V}ec: Distributional semantic modelling of human property knowledge",
    author = "Derby, Steven  and
      Miller, Paul  and
      Devereux, Barry",
    booktitle = "{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1595",
    doi = "10.18653/v1/D19-1595",
    pages = "5853--5859",
    abstract = "Feature norm datasets of human conceptual knowledge, collected in surveys of human volunteers, yield highly interpretable models of word meaning and play an important role in neurolinguistic research on semantic cognition. However, these datasets are limited in size due to practical obstacles associated with exhaustively listing properties for a large number of words. In contrast, the development of distributional modelling techniques and the availability of vast text corpora have allowed researchers to construct effective vector space models of word meaning over large lexicons. However, this comes at the cost of interpretable, human-like information about word meaning. We propose a method for mapping human property knowledge onto a distributional semantic space, which adapts the word2vec architecture to the task of modelling concept features. Our approach gives a measure of concept and feature affinity in a single semantic space, which makes for easy and efficient ranking of candidate human-derived semantic properties for arbitrary words. We compare our model with a previous approach, and show that it performs better on several evaluation tasks. Finally, we discuss how our method could be used to develop efficient sampling techniques to extend existing feature norm datasets in a reliable way.",
}


@inproceedings{misra2021typicality,
  title={Do language models learn typicality judgments from text?},
  author={Kanishka Misra and Allyson Ettinger and Julia Rayz},
  booktitle={{Proceedings of the 43rd Annual Conference of the Cognitive Science
Society}},
  year={2021}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "{Proceedings of EMNLP 2020: Demos}",
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "38--45"
}

@article{sanh2019distilbert,
  title={{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{lan2019albert,
  title={{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={{International Conference on Learning Representations}},
  year={2020}
}

@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=rygGQyrFvH}
}

@inproceedings{misra-etal-2020-exploring,
    title = "Exploring {BERT}{'}s Sensitivity to Lexical Cues using Tests from Semantic Priming",
    author = "Misra, Kanishka  and
      Ettinger, Allyson  and
      Rayz, Julia",
    booktitle = "{Findings of the Association for Computational Linguistics: EMNLP 2020}",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.415",
    doi = "10.18653/v1/2020.findings-emnlp.415",
    pages = "4625--4635",
    abstract = "Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows {``}priming{''}, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.",
}

@inproceedings{emerson-2020-goals,
    title = "What are the Goals of Distributional Semantics?",
    author = "Emerson, Guy",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.663",
    doi = "10.18653/v1/2020.acl-main.663",
    pages = "7436--7453",
    abstract = "Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks. However, assessing long-term progress requires explicit long-term goals. In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges. Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them. I conclude that, while linguistic insights can guide the design of model architectures, future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.",
}

@misc{mccarthy-1955-ai,
    title="A {P}roposal for the {D}arthmouth {S}ummer {R}esearch {Project on Artificial Intelligence}",
    author = "McCarthy, John and Minsky, Marvin Lee and Rochester, Natheniel and Shannon, Claude Elwood",
    year = "1955",
    url = "http://raysolomonoff.com/dartmouth/boxa/dart564props.pdf"
}

@article{ettinger2020bert,
  title={What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
  author={Ettinger, Allyson},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={34--48},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}

@article{federmeier1999rose,
  title={A rose by any other name: Long-term memory structure and sentence processing},
  author={Federmeier, Kara D and Kutas, Marta},
  journal={Journal of memory and Language},
  volume={41},
  number={4},
  pages={469--495},
  year={1999},
  publisher={Academic Press}
}

@article{mcclelland1981expectations,
  title={Expectations increase the benefit derived from parafoveal visual information in reading words aloud.},
  author={McClelland, James L and O'Regan, J Kevin},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={7},
  number={3},
  pages={634},
  year={1981},
  publisher={American Psychological Association}
}

@article{schwanenflugel1985influence,
  title={The influence of sentence constraint on the scope of facilitation for upcoming words},
  author={Schwanenflugel, Paula J and Shoben, Edward J},
  journal={Journal of Memory and Language},
  volume={24},
  number={2},
  pages={232--252},
  year={1985},
  publisher={Elsevier}
}

@incollection{schwanenflugel1991contextual,
  title={Contextual constraint and lexical processing},
  author={Schwanenflugel, Paula J},
  booktitle={Advances in psychology},
  volume={77},
  pages={23--45},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{faruqui2016problems,
  title={Problems With Evaluation of Word Embeddings Using Word Similarity Tasks},
  author={Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
  booktitle={Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP},
  pages={30--35},
  year={2016}
}

@inproceedings{clark2020electra,
title={{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={{International Conference on Learning Representations}},
year={2020},
url={https://openreview.net/forum?id=r1xMH1BtvB}
}

@inproceedings{yang2019xlnet,
  title={{XLNet: Generalized autoregressive pretraining for language understanding}},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5754--5764},
  year={2019}
}

@article{liu2019roberta,
  title={{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{hale-2001-probabilistic,
    title = "A Probabilistic {E}arley Parser as a Psycholinguistic Model",
    author = "Hale, John",
    booktitle = "Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2001",
    url = "https://www.aclweb.org/anthology/N01-1021",
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{goldberg2019assessing,
  title={Assessing BERT's syntactic abilities},
  author={Goldberg, Yoav},
  journal={arXiv preprint arXiv:1901.05287},
  year={2019}
}

@article{Talmor2019oLMpicsO,
  title={oLMpics - On what Language Model Pre-training Captures},
  author={Alon Talmor and Yanai Elazar and Yoav Goldberg and Jonathan Berant},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.13283}
}

@misc{kassner2019negated,
    title={Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly},
    author={Nora Kassner and Hinrich Schütze},
    year={2019},
    eprint={1911.03343},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{meyer1971facilitation,
  title={Facilitation in recognizing pairs of words: evidence of a dependence between retrieval operations.},
  author={Meyer, David E and Schvaneveldt, Roger W},
  journal={Journal of experimental psychology},
  volume={90},
  number={2},
  pages={227},
  year={1971},
  publisher={American Psychological Association}
}

@article{taylor1953cloze,
  title={“Cloze procedure”: A new tool for measuring readability},
  author={Taylor, Wilson L},
  journal={Journalism quarterly},
  volume={30},
  number={4},
  pages={415--433},
  year={1953},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{smith2013effect,
  title={The effect of word predictability on reading time is logarithmic},
  author={Smith, Nathaniel J and Levy, Roger},
  journal={Cognition},
  volume={128},
  number={3},
  pages={302--319},
  year={2013},
  publisher={Elsevier}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{wilcox2018rnn,
  title={What do RNN Language Models Learn about Filler--Gap Dependencies?},
  author={Wilcox, Ethan and Levy, Roger and Morita, Takashi and Futrell, Richard},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={211--221},
  year={2018}
}

@article{baayen2008mixed,
  title={Mixed-effects modeling with crossed random effects for subjects and items},
  author={Baayen, R Harald and Davidson, Douglas J and Bates, Douglas M},
  journal={Journal of memory and language},
  volume={59},
  number={4},
  pages={390--412},
  year={2008},
  publisher={Elsevier}
}

@article{duffy1989semantic,
  title={Semantic facilitation of lexical access during sentence processing.},
  author={Duffy, Susan A and Henderson, John M and Morris, Robin K},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={15},
  number={5},
  pages={791},
  year={1989},
  publisher={American Psychological Association}
}

@inproceedings{mostafazadeh2017lsdsem,
  title={{LSDSem 2017 Shared Task: The Story Cloze Test}},
  author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
  booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
  pages={46--51},
  year={2017}
}

@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2463--2473},
  year={2019}
}

@Book{mcnamara2005semantic,
  title={Semantic priming: Perspectives from memory and word recognition},
  author={McNamara, Timothy P},
  year=2005,
  publisher={Psychology Press}
}

@article{hill-etal-2015-simlex,
    title = "{S}im{L}ex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
    author = "Hill, Felix  and
      Reichart, Roi  and
      Korhonen, Anna",
    journal = "Computational Linguistics",
    volume = "41",
    number = "4",
    month = dec,
    year = "2015",
    url = "https://aclanthology.org/J15-4004",
    doi = "10.1162/COLI_a_00237",
    pages = "665--695",
}

@inproceedings{belinkov2017neural,
  title={What do Neural Machine Translation Models Learn about Morphology?},
  author={Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={861--872},
  year={2017}
}

@inproceedings{ettinger2016modeling,
  title={Modeling N400 amplitude using vector space models of word representation.},
  author={Ettinger, Allyson and Feldman, Naomi and Resnik, Philip and Phillips, Colin},
  year={2016},
  booktitle={Proceedings of the 38th Annual Meeting of the Cognitive Science Society}
}

@article{warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  publisher={MIT Press}
}

@inproceedings{huang-etal-2012-improving,
    title = "Improving Word Representations via Global Context and Multiple Word Prototypes",
    author = "Huang, Eric  and
      Socher, Richard  and
      Manning, Christopher  and
      Ng, Andrew",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-1092",
    pages = "873--882",
}


@inproceedings{ettinger-linzen-2016-evaluating,
    title = "Evaluating vector space models using human semantic priming results",
    author = "Ettinger, Allyson  and
      Linzen, Tal",
    booktitle = "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP}",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2513",
    doi = "10.18653/v1/W16-2513",
    pages = "72--77",
}

@inproceedings{auguste-etal-2017-evaluation,
    title = "Evaluation of word embeddings against cognitive processes: primed reaction times in lexical decision and naming tasks",
    author = "Auguste, Jeremy  and
      Rey, Arnaud  and
      Favre, Benoit",
    booktitle = "Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}",
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    pages = "21--26"
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={{Advances in Neural Information Processing Systems}},
  pages={5998--6008},
  year={2017}
}

@article{mcshane2017natural,
  title={Natural language understanding (NLU, not NLP) in cognitive systems},
  author={McShane, Marjorie},
  journal={AI Magazine},
  volume={38},
  number={4},
  pages={43--56},
  year={2017}
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel R",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}


@inproceedings{mikolov2011extensions,
  title={Extensions of recurrent neural network language model},
  author={Mikolov, Tom{\'a}{\v{s}} and Kombrink, Stefan and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5528--5531},
  year={2011},
  organization={IEEE}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@inproceedings{ettinger2016probing,
  title={Probing for semantic evidence of composition by means of simple classification tasks},
  author={Ettinger, Allyson and Elgohary, Ahmed and Resnik, Philip},
  booktitle={Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP},
  pages={134--139},
  year={2016}
}



@article{shwartz2019still,
  title={Still a pain in the neck: Evaluating text representations on lexical composition},
  author={Shwartz, Vered and Dagan, Ido},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={403--419},
  year={2019},
  publisher={MIT Press}
}

@article{haxby2012multivariate,
  title={Multivariate pattern analysis of fMRI: the early beginnings},
  author={Haxby, James V},
  journal={Neuroimage},
  volume={62},
  number={2},
  pages={852--855},
  year={2012},
  publisher={Elsevier}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{clark2019does,
  title={What Does BERT Look at? An Analysis of BERT’s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1724--1734},
  year={2014}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{taylor2010unintentional,
  title={An unintentional inference and ontological property defaults},
  author={Taylor, Julia M and Raskin, Victor and Hempelmann, Christian F and Attardo, Salvatore},
  booktitle={2010 IEEE International Conference on Systems, Man and Cybernetics},
  pages={3333--3339},
  year={2010},
  organization={IEEE}
}

@inproceedings{taylor2011understanding,
  title={{Understanding the unknown: Unattested input processing in natural language}},
  author={Taylor, Julia M and Raskin, Victor},
  booktitle={2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)},
  pages={94--101},
  year={2011},
  organization={IEEE}
}

@inproceedings{taylor2016conceptual,
  title={{Conceptual defaults in fuzzy ontology}},
  author={Taylor, Julia M and Raskin, Victor},
  booktitle={2016 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS)},
  pages={1--6},
  organization={IEEE},
  year = {2016}
}

@article{mandera2017explaining,
  title={Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting: A review and empirical validation},
  author={Mandera, Pawe{\l} and Keuleers, Emmanuel and Brysbaert, Marc},
  journal={Journal of Memory and Language},
  volume={92},
  pages={57--78},
  year={2017},
  publisher={Elsevier}
}

@book{jurafsky2020speech,
  title={{Speech \& Language Processing, 3rd Edition}},
  author={Jurafsky, Dan and Martin, James H},
  year={2020},
  url={https://web.stanford.edu/~jurafsky/slp3/}
}

@book{firth1968selected,
  title={A synopsis of linguistic theory 1930-1955.},
  author={Firth, John Rupert},
  year={1957},
  publisher={Studies in linguistic analysis},
  pages={1--32}
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis}
}

@inproceedings{darpa,
  title={{A DARPA Perpective on Artificial Intelligence}},
  author={Launchbury, John},
  url={https://www.darpa.mil/attachments/AIFull.pdf},
  year={2019}
}

@article{zadeh1965fuzzy,
  title={Fuzzy sets},
  author={Zadeh, Lotfi A},
  journal={Information and control},
  volume={8},
  number={3},
  pages={338--353},
  year={1965},
  publisher={Elsevier}
}

@inproceedings{taylor2011towards,
  title={{Towards computational guessing of unknown word meanings: The Ontological Semantic approach}},
  author={Taylor, Julia M and Raskin, Victor and Hempelmann, Christian F},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={33},
  year={2011}
}

@book{nirenburg2004ontological,
  title={{Ontological semantics}},
  author={Nirenburg, Sergei and Raskin, Victor},
  year={2004},
  publisher={MIT Press}
}

@inproceedings{taylor2010automatic,
  title={{On an automatic acquisition toolbox for ontologies and lexicons in ontological semantics}},
  author={Taylor, Julia M and Hempelmann, Christian F and Raskin, Victor},
  booktitle={ICAI 2010: Proceedings of the 2010 International Conference on Artificial Intelligence (Las Vegas NV, July 12-15, 2010)},
  pages={863--869},
  year={2010}
}

@inproceedings{raskin2010guessing,
  title={{Guessing vs. knowing: The two approaches to semantics in natural language processing}},
  author={Raskin, Victor and Hempelmann, Christian F and Taylor, Julia M},
  booktitle={Annual International Conference Dialogue 2010},
  pages={642--650},
  year={2010}
}

@inproceedings{raskin2009not,
  title={{The (not so) unbearable fuzziness of natural language: The ontological semantic way of computing with words}},
  author={Raskin, Victor and Taylor, Julia M},
  booktitle={2009 Annual Conference of the North American Fuzzy Information Processing Society},
  pages={1--6},
  year={2009},
  organization={IEEE}
}

@inproceedings{hempelmann2010application,
  title={{Application-guided ontological engineering}},
  author={Hempelmann, Christian F and Taylor, Julia M and Raskin, Victor},
  booktitle={ICAI 2010: Proceedings of the 2010 International Conference on Artificial Intelligence (Las Vegas NV, July 12-15, 2010)},
  pages={843--849},
  year={2010}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}



@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{collins1975spreading,
  title={A spreading-activation theory of semantic processing.},
  author={Collins, Allan M and Loftus, Elizabeth F},
  journal={Psychological review},
  volume={82},
  number={6},
  pages={407},
  year={1975},
  publisher={American Psychological Association}
}

@inproceedings{plaut1995semantic,
  title={Semantic and associative priming in a distributed attractor network},
  author={Plaut, David C},
  booktitle={Proceedings of the 17th annual conference of the cognitive science society},
  volume={17},
  number={2},
  pages={37--42},
  year={1995},
  organization={Pittsburgh, PA}
}

@inproceedings{sommerauer-fokkens-2018-firearms,
    title = "Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell",
    author = "Sommerauer, Pia  and
      Fokkens, Antske",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5430",
    doi = "10.18653/v1/W18-5430",
    pages = "276--286",
    abstract = "This paper presents an approach for investigating the nature of semantic information captured by word embeddings. We propose a method that extends an existing human-elicited semantic property dataset with gold negative examples using crowd judgments. Our experimental approach tests the ability of supervised classifiers to identify semantic features in word embedding vectors and compares this to a feature-identification method based on full vector cosine similarity. The idea behind this method is that properties identified by classifiers, but not through full vector comparison are captured by embeddings. Properties that cannot be identified by either method are not. Our results provide an initial indication that semantic properties relevant for the way entities interact (e.g. dangerous) are captured, while perceptual information (e.g. colors) is not represented. We conclude that, though preliminary, these results show that our method is suitable for identifying which properties are captured by embeddings.",
}

@inproceedings{weir2020probing,
  title={Probing neural language models for human tacit assumptions},
  author={Weir, Nathaniel and Poliak, Adam and Van Durme, Benjamin},
  year={2020},
  booktitle={{CogSci 2020}},
  pages={377--383},
  publisher={Cognitive Science Society}
}

@inproceedings{poliak-etal-2018-hypothesis,
    title = "Hypothesis Only Baselines in Natural Language Inference",
    author = "Poliak, Adam  and
      Naradowsky, Jason  and
      Haldar, Aparajita  and
      Rudinger, Rachel  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S18-2023",
    doi = "10.18653/v1/S18-2023",
    pages = "180--191",
    abstract = "We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution. Yet, through experiments on 10 distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets. Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.",
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{forbes2019neural,
  title={{Do Neural Language Representations Learn Physical Commonsense?}},
  author={Forbes, Maxwell and Holtzman, Ari and Choi, Yejin},
  year={2019},
  booktitle={{Proceedings of the 41st Annual Meeting of the Cognitive Science Society}}
}

@inproceedings{wu-palmer-1994-verb,
    title = {{Verb Semantics and Lexical Selection}},
    author = "Wu, Zhibiao  and
      Palmer, Martha",
    booktitle = {{32nd Annual Meeting of the Association for Computational Linguistics}},
    year = "1994",
    address = "Las Cruces, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P94-1019",
    doi = "10.3115/981732.981751",
    pages = "133--138",
}

@inproceedings{mccoy-etal-2019-right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, R Thomas  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
    abstract = "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
}

@inproceedings{niven-kao-2019-probing,
    title = "Probing Neural Network Comprehension of Natural Language Arguments",
    author = "Niven, Timothy  and
      Kao, Hung-Yu",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1459",
    doi = "10.18653/v1/P19-1459",
    pages = "4658--4664",
    abstract = "We are surprised to find that BERT{'}s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
}


@inproceedings{ravichander-etal-2020-systematicity,
    title = "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}",
    author = "Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Suleman, Kaheer  and
      Trischler, Adam  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics",
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.starsem-1.10",
    pages = "88--102",
    abstract = "Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: \textit{do probing studies shed light on systematic knowledge in BERT representations?} As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT {`}understands{'} a concept, and it cannot be expected to systematically generalize across applicable contexts.",
}


@inproceedings{ravichander-etal-2021-probing,
    title = "Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?",
    author = "Ravichander, Abhilasha  and
      Belinkov, Yonatan  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.295",
    pages = "3363--3377",
    abstract = "Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of {`}probing{'} tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.",
}


@inproceedings{lucy-gauthier-2017-distributional,
    title = "Are Distributional Representations Ready for the Real World? Evaluating Word Vectors for Grounded Perceptual Meaning",
    author = "Lucy, Li  and
      Gauthier, Jon",
    booktitle = "Proceedings of the First Workshop on Language Grounding for Robotics",
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2810",
    doi = "10.18653/v1/W17-2810",
    pages = "76--85",
    abstract = "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.",
}


@article{mcrae2005semantic,
  title={Semantic feature production norms for a large set of living and nonliving things},
  author={McRae, Ken and Cree, George S and Seidenberg, Mark S and McNorgan, Chris},
  journal={Behavior research methods},
  volume={37},
  number={4},
  pages={547--559},
  year={2005},
  publisher={Springer}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@inproceedings{mccann2017learned,
  title={Learned in translation: Contextualized word vectors},
  author={McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6294--6305},
  year={2017}
}

@inproceedings{warstadt2019investigating,
  title={Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs},
  author={Warstadt, Alex and Cao, Yu and Grosu, Ioana and Peng, Wei and Blix, Hagen and Nie, Yining and Alsop, Anna and Bordia, Shikha and Liu, Haokun and Parrish, Alicia and others},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2870--2880},
  year={2019}
}

@article{hutchison2003semantic,
  title={Is semantic priming due to association strength or feature overlap? A microanalytic review},
  author={Hutchison, Keith A},
  journal={Psychonomic bulletin \& review},
  volume={10},
  number={4},
  pages={785--813},
  year={2003},
  publisher={Springer}
}

@article{mcclelland1986parallel,
  title={Parallel distributed processing},
  author={McClelland, James L and Rumelhart, David E},
  journal={Explorations in the Microstructure of Cognition},
  volume={2},
  pages={216--271},
  year={1986},
  publisher={MIT Press Cambridge, Ma}
}

@article{quillian1967word,
  title={Word concepts: A theory and simulation of some basic semantic capabilities},
  author={Quillian, M Ross},
  journal={Behavioral science},
  volume={12},
  number={5},
  pages={410--430},
  year={1967},
  publisher={Wiley Online Library}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}


@inproceedings{taylor2010fuzzy,
  title={Fuzzy ontology for natural language},
  author={Taylor, Julia M and Raskin, Victor},
  booktitle={2010 Annual Meeting of the North American Fuzzy Information Processing Society},
  pages={1--6},
  year={2010},
  organization={IEEE}
}

@techreport{radfordimproving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  institution={Open AI},
  url={https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@inproceedings{conneau2018you,
  title={What you can cram into a single \$\&!\#* vector: Probing sentence embeddings for linguistic properties},
  author={Conneau, Alexis and Kruszewski, Germ{\'a}n and Lample, Guillaume and Barrault, Lo{\"\i}c and Baroni, Marco},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2126--2136},
  year={2018}
}

@article{hupkes2020compositionality,
  title={Compositionality Decomposed: How do Neural Networks Generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@inproceedings{wallace2019nlp,
  title={Do NLP Models Know Numbers? Probing Numeracy in Embeddings},
  author={Wallace, Eric and Wang, Yizhong and Li, Sujian and Singh, Sameer and Gardner, Matt},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5310--5318},
  year={2019}
}

@inproceedings{adi2017fine,
  author    = {Yossi Adi and
               Einat Kermany and
               Yonatan Belinkov and
               Ofer Lavi and
               Yoav Goldberg},
  title     = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction
               Tasks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  url       = {https://openreview.net/forum?id=BJh6Ztuxl}
}


@inproceedings{kim2019probing,
  title={Probing What Different NLP Tasks Teach Machines about Function Word Comprehension},
  author={Kim, Najoung and Patel, Roma and Poliak, Adam and Xia, Patrick and Wang, Alex and McCoy, Tom and Tenney, Ian and Ross, Alexis and Linzen, Tal and Van Durme, Benjamin and others},
  booktitle={Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (* SEM 2019)},
  pages={235--249},
  year={2019}
}

@inproceedings{hewitt2019designing,
  title={Designing and Interpreting Probes with Control Tasks},
  author={Hewitt, John and Liang, Percy},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2733--2743},
  year={2019}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@inproceedings{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={632--642},
  year={2015}
}

@article{dagan2013recognizing,
  title={Recognizing textual entailment: Models and applications},
  author={Dagan, Ido and Roth, Dan and Sammons, Mark and Zanzotto, Fabio Massimo},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={6},
  number={4},
  pages={1--220},
  year={2013},
  publisher={Morgan \& Claypool Publishers}
}

@inbook{raskin1987language,
  booktitle={{Language and Writing: Applications of Linguistics to Rhetoric and Composition}},
  author={Raskin, V. and Weiser, I.},
  isbn={9780893914059},
  lccn={87017753},
  series={Applications of Linguistics to Rhetoric and Composition},
  year={1987},
  title={Chapter 8},
  publisher={ABLEX Publishing Corporation}
}

@article{alishahi2019analyzing,
  title={Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop},
  author={Alishahi, Afra and Chrupa{\l}a, Grzegorz and Linzen, Tal},
  journal={Natural Language Engineering},
  volume={25},
  number={4},
  pages={543--557},
  year={2019},
  publisher={Cambridge University Press}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of {EMNLP 2014}},
  pages={1532--1543},
  year={2014}
}

@inproceedings{frank2019interaction,
  title={The interaction between structure and meaning in sentence comprehension: Recurrent neural networks and reading times},
  author={Frank, Stefan L and Hoeks, John},
  year = {2019},
  booktitle = {CogSci 2019},
  editor = {Ashok Goel, Colleen Seifert, Christian Freksa}
}

@inproceedings{van2018modeling,
  title={Modeling garden path effects without explicit hierarchical syntax.},
  author={van Schijndel, Marten and Linzen, Tal},
  booktitle = {CogSci 2018},
  editor = {Chuck Kalish, Martina Rau, Jerry Zhu, and Timothy Rogers},
  year = {2018}
}

@article{nieuwland2008truth,
  title={When the truth is not too hard to handle: An event-related potential study on the pragmatics of negation},
  author={Nieuwland, Mante S and Kuperberg, Gina R},
  journal={Psychological Science},
  volume={19},
  number={12},
  pages={1213--1218},
  year={2008},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{fischler1985brain,
  title={Brain potentials during sentence verification: Automatic aspects of comprehension},
  author={Fischler, Ira and Childers, Donald G and Achariyapaopan, Teera and Perry Jr, Nathan W},
  journal={Biological psychology},
  volume={21},
  number={2},
  pages={83--105},
  year={1985},
  publisher={Elsevier}
}



@inproceedings{frank-etal-2013-word,
    title = "Word surprisal predicts N400 amplitude during reading",
    author = "Frank, Stefan L.  and
      Otten, Leun J.  and
      Galli, Giulia  and
      Vigliocco, Gabriella",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P13-2152",
    pages = "878--883",
}
@article{john1990learning,
  title={Learning and applying contextual constraints in sentence comprehension},
  author={John, Mark F St and McClelland, James L},
  journal={Artificial intelligence},
  volume={46},
  number={1-2},
  pages={217--257},
  year={1990},
  publisher={Elsevier}
}

@article{rabovsky2018modelling,
  title={Modelling the N400 brain potential as change in a probabilistic representation of meaning},
  author={Rabovsky, Milena and Hansen, Steven S and McClelland, James L},
  journal={Nature Human Behaviour},
  volume={2},
  number={9},
  pages={693--705},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{brouwer2017neurocomputational,
  title={A neurocomputational model of the N400 and the P600 in language processing},
  author={Brouwer, Harm and Crocker, Matthew W and Venhuizen, Noortje J and Hoeks, John CJ},
  journal={Cognitive science},
  volume={41},
  pages={1318--1352},
  year={2017},
  publisher={Wiley Online Library}
}

@article{kutas1993company,
  title={In the company of other words: Electrophysiological evidence for single-word and sentence context effects},
  author={Kutas, Marta},
  journal={Language and cognitive processes},
  volume={8},
  number={4},
  pages={533--572},
  year={1993},
  publisher={Taylor \& Francis}
}

@article{bojanowski2017enriching,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press}
}

@inproceedings{misra2020cogsci,
  title={Exploring Lexical Relations in BERT using Semantic Priming},
  author={Kanishka Misra and Allyson Ettinger and Julia Rayz},
  booktitle={Proceedings of the 42nd Annual Conference of the Cognitive Science
Society},
  pages={1939},
  year={2020}
}

@book{murphy2004big,
  title={{The Big Book of Concepts}},
  author={Murphy, Gregory L},
  year={2002},
  publisher={MIT press}
}


@book{lenat1989cyc,
author = {Lenat, Douglas B. and Guha, R. V.},
title = {Building Large Knowledge-Based Systems; Representation and Inference in the Cyc Project},
year = {1989},
isbn = {0201517523},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
edition = {1st}
}

@book{jackendoff1983semantics,
  title={Semantics and cognition},
  author={Jackendoff, Ray},
  volume={8},
  year={1983},
  publisher={MIT Press}
}

@article{baroni-lenci-2010-distributional,
    title = "Distributional Memory: A General Framework for Corpus-Based Semantics",
    author = "Baroni, Marco  and
      Lenci, Alessandro",
    journal = "Computational Linguistics",
    volume = "36",
    number = "4",
    year = "2010",
    url = "https://aclanthology.org/J10-4006",
    doi = "10.1162/coli_a_00016",
    pages = "673--721",
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={IEEE}
}

@inproceedings{baroni-etal-2014-dont,
    title = "Don{'}t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    author = "Baroni, Marco  and
      Dinu, Georgiana  and
      Kruszewski, Germ{\'a}n",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1023",
    doi = "10.3115/v1/P14-1023",
    pages = "238--247",
}


@inproceedings{erk-2009-representing,
    title = "Representing words as regions in vector space",
    author = "Erk, Katrin",
    booktitle = "Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL}-2009)",
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W09-1109",
    pages = "57--65",
}

@article{landauer1997solution,
  title={A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
  author={Landauer, Thomas K and Dumais, Susan T},
  journal={Psychological review},
  volume={104},
  number={2},
  pages={211},
  year={1997},
  publisher={American Psychological Association}
}

@article{barsalou1983ad,
  title={Ad hoc categories},
  author={Barsalou, Lawrence W.},
  journal={Memory \& cognition},
  volume={11},
  number={3},
  pages={211--227},
  year={1983},
  publisher={Springer}
}

@article{casasanto2015all,
  title={All concepts are ad hoc concepts},
  author={Casasanto, Daniel and Lupyan, Gary},
  journal={The conceptual mind: New directions in the study of concepts},
  pages={543--566},
  year={2015}
}

@book{fodor1998concepts,
  title={Concepts: Where cognitive science went wrong},
  author={Fodor, Jerry A},
  year={1998},
  publisher={Oxford University Press}
}

@book{pustejovsky1998generative,
  title={The generative lexicon},
  author={Pustejovsky, James},
  year={1998},
  publisher={MIT Press}
}

@article{erk2016you,
  title={What do you know about an alligator when you know the company it keeps?},
  author={Erk, Katrin},
  journal={Semantics and Pragmatics},
  volume={9},
  number={17},
  pages={1--63},
  year={2016},
  url={http://dx.doi.org/10.3765/sp.9.17}
}

@book{murphy2010lexical,
  title={Lexical meaning},
  author={Murphy, M Lynne},
  year={2010},
  publisher={Cambridge University Press}
}

@inproceedings{erk-2009-supporting,
    title = "Supporting inferences in semantic space: representing words as regions",
    author = "Erk, Katrin",
    booktitle = "Proceedings of the Eight International Conference on Computational Semantics",
    year = "2009",
    address = "Tilburg, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W09-3711",
    pages = "104--115",
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.463",
    doi = "10.18653/v1/2020.acl-main.463",
    pages = "5185--5198",
    abstract = "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",
}


@misc{koller2016topbottom,
    title = "Top-down and bottom-up views on success in semantics",
    author = "Koller, Alexander",
    year = "2016",
    url = "https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxzdGFyc2VtMjAxNnxneDoyNTcyY2VhZmYyYWVkZmFk"
}


@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3111--3119},
  year={2013}
}

@article{hutchison2013semantic,
  title={The semantic priming project},
  author={Hutchison, Keith A and Balota, David A and Neely, James H and Cortese, Michael J and Cohen-Shikora, Emily R and Tse, Chi-Shing and Yap, Melvin J and Bengson, Jesse J and Niemeyer, Dale and Buchanan, Erin},
  journal={Behavior Research Methods},
  volume={45},
  number={4},
  pages={1099--1114},
  year={2013},
  publisher={Springer}
}

@article{navigli2009word,
  title={Word sense disambiguation: A survey},
  author={Navigli, Roberto},
  journal={ACM computing surveys (CSUR)},
  volume={41},
  number={2},
  pages={1--69},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{camacho2018word,
  title={From word to sense embeddings: A survey on vector representations of meaning},
  author={Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
  journal={Journal of Artificial Intelligence Research},
  volume={63},
  pages={743--788},
  year={2018},
  url={https://www.jair.org/index.php/jair/article/view/11259}
}

@inproceedings{miller-etal-1993-semantic,
    title = "A Semantic Concordance",
    author = "Miller, George A.  and
      Leacock, Claudia  and
      Tengi, Randee  and
      Bunker, Ross T.",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",
    year = "1993",
    url = "https://aclanthology.org/H93-1061",
}

@article{camacho2016nasari,
  title={Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities},
  author={Camacho-Collados, Jos{\'e} and Pilehvar, Mohammad Taher and Navigli, Roberto},
  journal={Artificial Intelligence},
  volume={240},
  pages={36--64},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{li-jurafsky-2015-multi,
    title = "Do Multi-Sense Embeddings Improve Natural Language Understanding?",
    author = "Li, Jiwei  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1200",
    doi = "10.18653/v1/D15-1200",
    pages = "1722--1732",
}

@article{klepousniotou2012not,
  title={Not all ambiguous words are created equal: An EEG investigation of homonymy and polysemy},
  author={Klepousniotou, Ekaterini and Pike, G Bruce and Steinhauer, Karsten and Gracco, Vincent},
  journal={Brain and language},
  volume={123},
  number={1},
  pages={11--21},
  year={2012},
  publisher={Elsevier}
}

@article{macgregor2015sustained,
  title={Sustained meaning activation for polysemous but not homonymous words: Evidence from EEG},
  author={MacGregor, Lucy J and Bouwsema, Jennifer and Klepousniotou, Ekaterini},
  journal={Neuropsychologia},
  volume={68},
  pages={126--138},
  year={2015},
  publisher={Elsevier}
}

@article{lake2021word,
  title={Word meaning in minds and machines.},
  author={Lake, Brenden M and Murphy, Gregory L},
  journal={Psychological Review},
  year={2021},
  publisher={American Psychological Association}
}

@inproceedings{nair-etal-2020-contextualized,
    title = "Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge",
    author = "Nair, Sathvik  and
      Srinivasan, Mahesh  and
      Meylan, Stephan",
    booktitle = "Proceedings of the Workshop on the Cognitive Aspects of the Lexicon",
    month = dec,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.cogalex-1.16",
    pages = "129--141",
    abstract = "Understanding context-dependent variation in word meanings is a key aspect of human language comprehension supported by the lexicon. Lexicographic resources (e.g., WordNet) capture only some of this context-dependent variation; for example, they often do not encode how closely senses, or discretized word meanings, are related to one another. Our work investigates whether recent advances in NLP, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as polysemy and homonymy. We collect data from a behavioral, web-based experiment, in which participants provide judgments of the relatedness of multiple WordNet senses of a word in a two-dimensional spatial arrangement task. We find that participants{'} judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space. Specifically, homonymous senses (e.g., bat as mammal vs. bat as sports equipment) are reliably more distant from one another in the embedding space than polysemous ones (e.g., chicken as animal vs. chicken as meat). Our findings point towards the potential utility of continuous-space representations of sense meanings.",
}


@inproceedings{garcia-2021-exploring,
    title = "Exploring the Representation of Word Meanings in Context: {A} Case Study on Homonymy and Synonymy",
    author = "Garcia, Marcos",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.281",
    doi = "10.18653/v1/2021.acl-long.281",
    pages = "3625--3640",
    abstract = "This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.",
}

@inproceedings{baroni-lenci-2011-blessed,
    title = "How we {BLESS}ed distributional semantic evaluation",
    author = "Baroni, Marco  and
      Lenci, Alessandro",
    booktitle = "Proceedings of the {GEMS} 2011 Workshop on {GE}ometrical Models of Natural Language Semantics",
    month = jul,
    year = "2011",
    address = "Edinburgh, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2501",
    pages = "1--10",
}

@article{kotlerman2010directional,
  title={Directional distributional similarity for lexical inference},
  author={Kotlerman, Lili and Dagan, Ido and Szpektor, Idan and Zhitomirsky-Geffet, Maayan},
  journal={Natural Language Engineering},
  volume={16},
  number={4},
  pages={359--389},
  year={2010},
  publisher={Cambridge University Press},
  url={https://www.cambridge.org/core/journals/natural-language-engineering/article/directional-distributional-similarity-for-lexical-inference/666EA64B78F0A559FA3B1FC5DAA7B054}
}

@inproceedings{weeds-etal-2014-learning,
    title = "Learning to Distinguish Hypernyms and Co-Hyponyms",
    author = "Weeds, Julie  and
      Clarke, Daoud  and
      Reffin, Jeremy  and
      Weir, David  and
      Keller, Bill",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://aclanthology.org/C14-1212",
    pages = "2249--2259",
}

@book{murphy2003semantic,
  title={Semantic relations and the lexicon: Antonymy, synonymy and other paradigms},
  author={Murphy, M Lynne},
  year={2003},
  publisher={Cambridge University Press}
}

@incollection{tenenbaum2007theory,
  title = "Theory-based Bayesian Models of Inductive Reasoning",
  author      = "Tenenbaum, Joshua B. and Kemp, Charles and Shafto, Patrick",
  editor      = "Feeney, Aidan and Heit, Evan",
  booktitle   = "Inductive Reasoning: Experimental, Developmental, and Computational Approaches",
  publisher   = "Cambridge University Press",
  year        = 2007,
  pages       = "167-204",
  chapter     = 7,
}

@incollection{chater2011inductive,
  title={Inductive logic and empirical psychology},
  author={Chater, Nick and Oaksford, Mike and Hahn, Ulrike and Heit, Evan},
  editor={Gabbay, D. M. and Hartmann, S. and Woods, J.},
  booktitle={{Handbook of the History of Logic}},
  volume={10},
  pages={553--624},
  year={2011},
  publisher={Elsevier}
}

@book{grice1989studies,
  title={Studies in the Way of Words},
  author={Grice, Paul},
  year={1989},
  publisher={Harvard University Press}
}

@mastersthesis{misra2020exploring,
  title={Exploring Lexical Sensitivities in Word Prediction Models: A case study on BERT},
  author={Misra, Kanishka},
  year={2020},
  school={Purdue University Graduate School}
}

@article{osherson1990category,
  title={{Category-based Induction}},
  author={Osherson, Daniel N and Smith, Edward E and Wilkie, Ormond and Lopez, Alejandro and Shafir, Eldar},
  journal={Psychological Review},
  volume={97},
  number={2},
  pages={185},
  year={1990},
  publisher={American Psychological Association}
}

@inproceedings{bisk-etal-2020-experience,
    title = "Experience Grounds Language",
    author = "Bisk, Yonatan  and
      Holtzman, Ari  and
      Thomason, Jesse  and
      Andreas, Jacob  and
      Bengio, Yoshua  and
      Chai, Joyce  and
      Lapata, Mirella  and
      Lazaridou, Angeliki  and
      May, Jonathan  and
      Nisnevich, Aleksandr  and
      Pinto, Nicolas  and
      Turian, Joseph",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.703",
    doi = "10.18653/v1/2020.emnlp-main.703",
    pages = "8718--8735"
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@book{sparck1986synonymy,
  title={Synonymy and semantic classification},
  author={Spärck-Jones, Karen},
  year={1964},
  publisher={Edinburgh University Press}
}

@article{heit_properties_2000,
	title = {Properties of inductive reasoning},
	volume = {7},
	issn = {1531-5320},
	language = {en},
	number = {4},
	urldate = {2020-12-24},
	journal = {Psychonomic Bulletin \& Review},
	author = {Heit, Evan},
	year = {2000},
	pages = {569--592}
}

@book{feeney2007inductive,
  title={Inductive reasoning: Experimental, developmental, and computational approaches.},
  author={Feeney, Aidan Ed and Heit, Evan Ed},
  year={2007},
  publisher={Cambridge University Press}
}

@article{hayes2018inductive,
  title={Inductive reasoning 2.0},
  author={Hayes, Brett K and Heit, Evan},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={9},
  number={3},
  pages={e1459},
  year={2018},
  publisher={Wiley Online Library}
}

@article{rips1975inductive,
  title={Inductive judgments about natural categories},
  author={Rips, Lance J},
  journal={{Journal of Verbal Learning and Verbal Behavior}},
  volume={14},
  number={6},
  pages={665--681},
  year={1975},
  publisher={Elsevier}
}

@article{kemp2014taxonomy,
  title={{A Taxonomy of Inductive Problems}},
  author={Kemp, Charles and Jern, Alan},
  journal={{Psychonomic Bulletin \& Review}},
  volume={21},
  number={1},
  pages={23--46},
  year={2014},
  publisher={Springer}
}

@article{misra2021prelim1,
    title={On Existing Computational Approaches to Understand Word Meaning},
    author={Misra, Kanishka},
    year={2021},
    journal={Internal Report},
    publisher={Purdue University}
}

@article{misra2021prelim2,
    title={An analysis of evaluation methods and metrics for word-meaning understanding},
    author={Misra, Kanishka},
    year={2021},
    journal={Internal Report},
    publisher={Purdue University}
}

@article{lupyan2019words,
  title={From words-as-mappings to words-as-cues: The role of language in semantic knowledge},
  author={Lupyan, Gary and Lewis, Molly},
  journal={Language, Cognition and Neuroscience},
  volume={34},
  number={10},
  pages={1319--1337},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{lmertest,
    title = {{lmerTest} Package: Tests in Linear Mixed Effects Models},
    author = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B.
      Christensen},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {82},
    number = {13},
    pages = {1--26},
    doi = {10.18637/jss.v082.i13},
  }

@article{lme4,
    title = {Fitting Linear Mixed-Effects Models Using {lme4}},
    author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and
      Steve Walker},
    journal = {Journal of Statistical Software},
    year = {2015},
    volume = {67},
    number = {1},
    pages = {1--48},
    doi = {10.18637/jss.v067.i01},
  }

@inproceedings{kim-smolensky-2021-testing,
    title = "Testing for Grammatical Category Abstraction in Neural Language Models",
    author = "Kim, Najoung  and
      Smolensky, Paul",
    booktitle = {{Proceedings of the Society for Computation in Linguistics 2021}},
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.scil-1.59",
    pages = "467--470",
}

@inproceedings{loshchilov2018decoupled,
  title={{Decoupled Weight Decay Regularization}},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={{International Conference on Learning Representations}},
  year={2018}
}

@inproceedings{bhagavatula2019abductive,
  title={{Abductive Commonsense Reasoning}},
  author={Bhagavatula, Chandra and Le Bras, Ronan and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Wen-tau and Choi, Yejin},
  booktitle={{International Conference on Learning Representations}},
  year={2020}
}

@article{saxe2019mathematical,
  title={A mathematical theory of semantic development in deep neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={23},
  pages={11537--11546},
  year={2019},
  publisher={National Acad Sciences}
}

@article{devereux2014centre,
  title={{The Centre for Speech, Language and the Brain (CSLB) concept property norms}},
  author={Devereux, Barry J and Tyler, Lorraine K and Geertzen, Jeroen and Randall, Billi},
  journal={{Behavior Research Methods}},
  volume={46},
  number={4},
  pages={1119--1127},
  year={2014},
  publisher={Springer}
}

@article{kemp2009structured,
  title={{Structured Statistical Models of Inductive Reasoning}},
  author={Kemp, Charles and Tenenbaum, Joshua B},
  journal={{Psychological Review}},
  volume={116},
  number={1},
  pages={20},
  year={2009},
  publisher={American Psychological Association}
}

@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017},
  url={https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14972}
}

@inproceedings{gordon2013reporting,
  title={Reporting bias and knowledge acquisition},
  author={Gordon, Jonathan and Van Durme, Benjamin},
  booktitle={Proceedings of the 2013 workshop on Automated knowledge base construction},
  pages={25--30},
  year={2013},
  url={https://dl.acm.org/doi/abs/10.1145/2509558.2509563?casa_token=mE3LH0NgZXYAAAAA:jV9pdGKqpOSLdftVM3UudHk0sa9nhH_xUspKq9oeBYEnQ9FK-yDUCenVi9ofiqGHqSL0eNnqVIgKvA}
}

@inproceedings{linzen-2020-accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.465",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
}

@book{margolis1999concepts,
  title={Concepts: core readings},
  author={Margolis, Eric and Laurence, Stephen and others},
  year={1999},
  publisher={Mit Press}
}

@article{margolis2015conceptual,
  title={The Conceptual Mind: New Directions in the Study of Concepts},
  author={Margolis, Eric and Laurence, Stephen},
  year={2015},
  publisher={The MIT Press}
}

@book{carey1985conceptual,
  title={Conceptual Change in Childhood},
  author={S. Carey},
  year={1985},
  publisher={MIT press}
}

@article{feeney2018forty,
  title={Forty years of progress on category-based inductive reasoning.},
  author={Feeney, Aidan},
  year={2018},
  publisher={Routledge/Taylor \& Francis Group}
}

@article{collins1969retrieval,
  title={Retrieval time from semantic memory},
  author={Collins, Allan M and Quillian, M Ross},
  journal={Journal of verbal learning and verbal behavior},
  volume={8},
  number={2},
  pages={240--247},
  year={1969},
  publisher={Elsevier}
}

@article{xu2007word,
  title={Word learning as Bayesian inference.},
  author={Xu, Fei and Tenenbaum, Joshua B},
  journal={Psychological review},
  volume={114},
  number={2},
  pages={245},
  year={2007},
  publisher={American Psychological Association}
}

@article{rosch1976basic,
  title={Basic objects in natural categories},
  author={Rosch, Eleanor and Mervis, Carolyn B and Gray, Wayne D and Johnson, David M and Boyes-Braem, Penny},
  journal={Cognitive psychology},
  volume={8},
  number={3},
  pages={382--439},
  year={1976},
  publisher={Elsevier}
}

@article{rosch1978cognition,
  title={Cognition and categorization},
  author={Rosch, Eleanor and Lloyd, Barbara Bloom},
  year={1978},
  publisher={Citeseer}
}

@article{tversky1983extensional,
  title={Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Psychological review},
  volume={90},
  number={4},
  pages={293},
  year={1983},
  publisher={American Psychological Association}
}

@article{hayes2010inductive,
  title={Inductive reasoning},
  author={Hayes, Brett K and Heit, Evan and Swendsen, Haruka},
  journal={Wiley interdisciplinary reviews: Cognitive science},
  volume={1},
  number={2},
  pages={278--292},
  year={2010},
  publisher={Wiley Online Library}
}

@article{sloman1993feature,
  title={Feature-based induction},
  author={Sloman, Steven A},
  journal={Cognitive psychology},
  volume={25},
  number={2},
  pages={231--280},
  year={1993},
  publisher={Elsevier}
}

@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}

@inproceedings{davison-etal-2019-commonsense,
    title = "Commonsense Knowledge Mining from Pretrained Models",
    author = "Davison, Joe  and
      Feldman, Joshua  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1109",
    doi = "10.18653/v1/D19-1109",
    pages = "1173--1178",
    abstract = "Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple{'}s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.",
}

@inproceedings{porada-etal-2021-modeling,
    title = "Modeling Event Plausibility with Consistent Conceptual Abstraction",
    author = "Porada, Ian  and
      Suleman, Kaheer  and
      Trischler, Adam  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.138",
    doi = "10.18653/v1/2021.naacl-main.138",
    pages = "1732--1743",
    abstract = "Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models{---}most recently pre-trained, Transformer language models{---}have demonstrated improvements in modeling event plausibility, their performance still falls short of humans{'}. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that {``}a person breathing{''} is plausible while {``}a dentist breathing{''} is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements.",
}

@inproceedings{salazar-etal-2020-masked,
    title = "Masked Language Model Scoring",
    author = "Salazar, Julian  and
      Liang, Davis  and
      Nguyen, Toan Q.  and
      Kirchhoff, Katrin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.240",
    doi = "10.18653/v1/2020.acl-main.240",
    pages = "2699--2712",
    abstract = "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model{'}s WER by 30{\%} relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL{'}s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",
}

@inproceedings{wang-cho-2019-bert,
    title = "{BERT} has a Mouth, and It Must Speak: {BERT} as a {M}arkov Random Field Language Model",
    author = "Wang, Alex  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-2304",
    doi = "10.18653/v1/W19-2304",
    pages = "30--36",
    abstract = "We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.",
}

@inproceedings{sap2020commonsense,
  title={Commonsense reasoning for natural language processing},
  author={Sap, Maarten and Shwartz, Vered and Bosselut, Antoine and Choi, Yejin and Roth, Dan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
  pages={27--33},
  year={2020}
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@inproceedings{bosselut-etal-2019-comet,
    title = "{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction",
    author = "Bosselut, Antoine  and
      Rashkin, Hannah  and
      Sap, Maarten  and
      Malaviya, Chaitanya  and
      Celikyilmaz, Asli  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1470",
    doi = "10.18653/v1/P19-1470",
    pages = "4762--4779",
    abstract = "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
}


@article{murphy1993theories,
  title={Theories and concept formation.},
  author={Murphy, Gregory L},
  year={1993},
  publisher={Academic Press}
}

@article{medin2003relevance,
  title={A relevance theory of induction},
  author={Medin, Douglas L and Coley, John D and Storms, Gert and Hayes, Brett L},
  journal={Psychonomic Bulletin \& Review},
  volume={10},
  number={3},
  pages={517--532},
  year={2003},
  publisher={Springer}
}

@article{murphy1985role,
  title={The role of theories in conceptual coherence.},
  author={Murphy, Gregory L and Medin, Douglas L},
  journal={Psychological review},
  volume={92},
  number={3},
  pages={289},
  year={1985},
  publisher={American Psychological Association}
}

@article{heit1994similarity,
  title={Similarity and property effects in inductive reasoning.},
  author={Heit, Evan and Rubinstein, Joshua},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={20},
  number={2},
  pages={411},
  year={1994},
  publisher={American Psychological Association}
}

@article{gelman1986categories,
  title={Categories and induction in young children},
  author={Gelman, Susan A and Markman, Ellen M},
  journal={Cognition},
  volume={23},
  number={3},
  pages={183--209},
  year={1986},
  publisher={Elsevier}
}

@article{heibeck1987word,
  title={Word learning in children: An examination of fast mapping},
  author={Heibeck, Tracy H and Markman, Ellen M},
  journal={Child development},
  pages={1021--1034},
  year={1987},
  publisher={JSTOR}
}

@article{spelke1990principles,
  title={Principles of object perception},
  author={Spelke, Elizabeth S},
  journal={Cognitive science},
  volume={14},
  number={1},
  pages={29--56},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{shwartz-choi-2020-neural,
    title = "Do Neural Language Models Overcome Reporting Bias?",
    author = "Shwartz, Vered  and
      Choi, Yejin",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.605",
    doi = "10.18653/v1/2020.coling-main.605",
    pages = "6863--6870",
    abstract = "Mining commonsense knowledge from corpora suffers from reporting bias, over-representing the rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent pre-trained language models overcome this issue. We find that while their generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus.",
}

@article{rosch1975cognitive,
  title={Cognitive representations of semantic categories.},
  author={Rosch, Eleanor},
  journal={Journal of experimental psychology: General},
  volume={104},
  number={3},
  pages={192},
  year={1975},
  publisher={American Psychological Association}
}

@article{nosofsky1986attention,
  title={Attention, similarity, and the identification--categorization relationship.},
  author={Nosofsky, Robert M},
  journal={Journal of experimental psychology: General},
  volume={115},
  number={1},
  pages={39},
  year={1986},
  publisher={American Psychological Association}
}

@inproceedings{da-kasai-2019-cracking,
    title = "Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations",
    author = "Da, Jeff  and
      Kasai, Jungo",
    booktitle = {{Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing}},
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6001",
    doi = "10.18653/v1/D19-6001",
    pages = "1--12",
    abstract = "Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT{'}s commonsense representation abilities. First, we probe BERT{'}s ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT{'}s pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.",
}

@article{kemp2011inductive,
  title={Inductive reasoning about chimeric creatures},
  author={Kemp, Charles},
  journal={{Advances in Neural Information Processing Systems}},
  volume={24},
  pages={316--324},
  year={2011}
}

@article{elman2004alternative,
  title={An alternative view of the mental lexicon},
  author={Elman, Jeffrey L},
  journal={Trends in cognitive sciences},
  volume={8},
  number={7},
  pages={301--306},
  year={2004},
  publisher={Elsevier}
}


@book{rogers2004semantic,
  title={Semantic cognition: A parallel distributed processing approach},
  author={Rogers, Timothy T and McClelland, James L},
  year={2004},
  publisher={MIT press}
}

@inproceedings{vyas-carpuat-2017-detecting,
    title = "Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection",
    author = "Vyas, Yogarshi  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-1004",
    doi = "10.18653/v1/S17-1004",
    pages = "33--43",
    abstract = "We introduce WHiC, a challenging testbed for detecting hypernymy, an asymmetric relation between words. While previous work has focused on detecting hypernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts. WHiC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context. We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.",
}


@inproceedings{santus-etal-2015-evalution,
    title = "{EVAL}ution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models",
    author = "Santus, Enrico  and
      Yung, Frances  and
      Lenci, Alessandro  and
      Huang, Chu-Ren",
    booktitle = "Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4208",
    doi = "10.18653/v1/W15-4208",
    pages = "64--69",
}

@inproceedings{santus-etal-2016-cogalex,
    title = "The {C}og{AL}ex-{V} Shared Task on the Corpus-Based Identification of Semantic Relations",
    author = "Santus, Enrico  and
      Gladkova, Anna  and
      Evert, Stefan  and
      Lenci, Alessandro",
    booktitle = "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-5309",
    pages = "69--79",
    abstract = "The shared task of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V) aims at providing a common benchmark for testing current corpus-based methods for the identification of lexical semantic relations (synonymy, antonymy, hypernymy, part-whole meronymy) and at gaining a better understanding of their respective strengths and weaknesses. The shared task uses a challenging dataset extracted from EVALution 1.0, which contains word pairs holding the above-mentioned relations as well as semantically unrelated control items (random). The task is split into two subtasks: (i) identification of related word pairs vs. unrelated ones; (ii) classification of the word pairs according to their semantic relation. This paper describes the subtasks, the dataset, the evaluation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at \url{https://sites.google.com/site/cogalex2016/home/shared-task}.",
}



@inproceedings{snow-etal-2007-learning,
    title = "Learning to Merge Word Senses",
    author = "Snow, Rion  and
      Prakash, Sushant  and
      Jurafsky, Daniel  and
      Ng, Andrew Y.",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1107",
    pages = "1005--1014",
}

@article{zipf1945meaning,
  title={The meaning-frequency relationship of words},
  author={Zipf, George Kingsley},
  journal={The Journal of general psychology},
  volume={33},
  number={2},
  pages={251--256},
  year={1945},
  publisher={Taylor \& Francis}
}


@inproceedings{raganato-etal-2017-word,
    title = "Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison",
    author = "Raganato, Alessandro  and
      Camacho-Collados, Jose  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-1010",
    pages = "99--110",
    abstract = "Word Sense Disambiguation is a long-standing task in Natural Language Processing, lying at the core of human language understanding. However, the evaluation of automatic systems has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that supervised systems clearly outperform knowledge-based models. Among the supervised systems, a linear classifier trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting neural networks on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.",
}


@inproceedings{pilehvar-etal-2017-towards,
    title = "Towards a Seamless Integration of Word Senses into Downstream {NLP} Applications",
    author = "Pilehvar, Mohammad Taher  and
      Camacho-Collados, Jose  and
      Navigli, Roberto  and
      Collier, Nigel",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1170",
    doi = "10.18653/v1/P17-1170",
    pages = "1857--1869",
    abstract = "Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.",
}


@inproceedings{neelakantan-etal-2014-efficient,
    title = "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space",
    author = "Neelakantan, Arvind  and
      Shankar, Jeevan  and
      Passos, Alexandre  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1113",
    doi = "10.3115/v1/D14-1113",
    pages = "1059--1069",
}



%%GENERATED BY ZOTERO
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = {{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}},
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{mostafazadeh-etal-2016-corpus,
    title = "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    author = "Mostafazadeh, Nasrin  and
      Chambers, Nathanael  and
      He, Xiaodong  and
      Parikh, Devi  and
      Batra, Dhruv  and
      Vanderwende, Lucy  and
      Kohli, Pushmeet  and
      Allen, James",
    booktitle = "Proceedings of {NAACL-HLT} 2016",
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    pages = "839--849",
}

@article{ettingerWhatBERTNot2019,
  title = {What {{BERT}} Is Not: {{Lessons}} from a New Suite of Psycholinguistic Diagnostics for Language Models},
  shorttitle = {What {{BERT}} Is Not},
  author = {Ettinger, Allyson},
  year = {2019},
  abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation.},
  archivePrefix = {arXiv},
  eprint = {1907.13528},
  eprinttype = {arxiv},
  file = {/home/kanishka/Zotero/storage/PA2QYMFP/Ettinger - 2019 - What BERT is not Lessons from a new suite of psyc.pdf;/home/kanishka/Zotero/storage/8NRCDJKE/1907.html},
  journal = {arXiv:1907.13528 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{zhu2015aligning,
  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},
  pages={19--27},
  year={2015},
  organization={IEEE}
}

@article{frank2011insensitivity,
  title={Insensitivity of the human sentence-processing system to hierarchical structure},
  author={Frank, Stefan L and Bod, Rens},
  journal={Psychological science},
  volume={22},
  number={6},
  pages={829--834},
  year={2011},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{fischler1983brain,
  title={Brain potentials related to stages of sentence verification},
  author={Fischler, Ira and Bloom, Paul A and Childers, Donald G and Roucos, Salim E and Perry Jr, Nathan W},
  journal={Psychophysiology},
  volume={20},
  number={4},
  pages={400--409},
  year={1983},
  publisher={Wiley Online Library}
}

@article{chow2016bag,
  title={A “bag-of-arguments” mechanism for initial verb predictions},
  author={Chow, Wing-Yee and Smith, Cybelle and Lau, Ellen and Phillips, Colin},
  journal={Language, Cognition and Neuroscience},
  volume={31},
  number={5},
  pages={577--596},
  year={2016},
  publisher={Taylor \& Francis}
}

@inproceedings{tenney2019you,
  title={What do you learn from context? Probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel and Das, Dipanjan and others},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}

@inproceedings{veldhoen2016diagnostic,
  title={Diagnostic Classifiers: Revealing how Neural Networks Process Hierarchical Structure},
  author={Veldhoen, S and Hupkes, D and Zuidema, W and others},
  booktitle={CEUR Workshop Proceedings},
  volume={1773},
  number={6},
  year={2016}
}

@inproceedings{ettinger2018assessing,
  title={Assessing Composition in Sentence Vector Representations},
  author={Ettinger, Allyson and Elgohary, Ahmed and Phillips, Colin and Resnik, Philip},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={1790--1801},
  year={2018}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{wilcox2019hierarchical,
  title={Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations},
  author={Wilcox, Ethan and Levy, Roger and Futrell, Richard},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={181--190},
  year={2019}
}

@inproceedings{futrell2019neural,
    title = "Neural language models as psycholinguistic subjects: Representations of syntactic state",
    author = "Futrell, Richard  and
      Wilcox, Ethan  and
      Morita, Takashi  and
      Qian, Peng  and
      Ballesteros, Miguel  and
      Levy, Roger",
    booktitle = "Proceedings of {NAACL-HLT 2019}",

    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "32--42",
    abstract = "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
}

@article{duffy1989semantic,
  title={Semantic facilitation of lexical access during sentence processing.},
  author={Duffy, Susan A and Henderson, John M and Morris, Robin K},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={15},
  number={5},
  pages={791},
  year={1989},
  publisher={American Psychological Association}
}

@article{petten1993comparison,
  title={A comparison of lexical and sentence-level context effects in event-related potentials},
  author={Petten, Cyma Van},
  journal={Language and Cognitive Processes},
  volume={8},
  number={4},
  pages={485--531},
  year={1993},
  publisher={Taylor \& Francis}
}

@Manual{rcoreteam,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019}
  }

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@article{kruskal1952use,
  title={Use of ranks in one-criterion variance analysis},
  author={Kruskal, William H and Wallis, W Allen},
  journal={Journal of the American statistical Association},
  volume={47},
  number={260},
  pages={583--621},
  year={1952},
  publisher={Taylor \& Francis Group}
}


@inproceedings{van2018neural,
  title={A Neural Model of Adaptation in Reading},
  author={van Schijndel, Marten and Linzen, Tal},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={4704--4710},
  year={2018}
}

@inproceedings{gulordavaColorlessGreenRecurrent2018,
  title = {Colorless {{Green Recurrent Networks Dream Hierarchically}}},
  booktitle = {{Proceedings of NAACL-HLT} 2018},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  pages = {1195--1205},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},

  abstract = {Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (``The colorless green ideas I ate with the chair sleep furiously''), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
  file = {/home/kanishka/Zotero/storage/S8KJGR3P/Gulordava et al. - 2018 - Colorless Green Recurrent Networks Dream Hierarchi.pdf}
}

@article{kutas1984brain,
  title={Brain potentials during reading reflect word expectancy and semantic association},
  author={Kutas, Marta and Hillyard, Steven A},
  journal={Nature},
  volume={307},
  number={5947},
  pages={161--163},
  year={1984},
  publisher={Nature Publishing Group}
}

@article{kutas2011thirty,
  title={Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)},
  author={Kutas, Marta and Federmeier, Kara D},
  journal={Annual review of psychology},
  volume={62},
  pages={621--647},
  year={2011},
  publisher={Annual Reviews}
}

@article{kutas1980reading,
  title={Reading senseless sentences: Brain potentials reflect semantic incongruity},
  author={Kutas, Marta and Hillyard, Steven A},
  journal={Science},
  volume={207},
  number={4427},
  pages={203--205},
  year={1980},
  publisher={American Association for the Advancement of Science}
}

@article{hutchison2001great,
  title={With great expectations, can two" wrongs" prime a" right"?},
  author={Hutchison, Keith A and Neely, James H and Johnson, Jeffrey D},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={27},
  number={6},
  pages={1451},
  year={2001},
  publisher={American Psychological Association}
}

@article{hutchisonSemanticPrimingProject2013,
  title = {The Semantic Priming Project},
  author = {Hutchison, Keith A. and Balota, David A. and Neely, James H. and Cortese, Michael J. and {Cohen-Shikora}, Emily R. and Tse, Chi-Shing and Yap, Melvin J. and Bengson, Jesse J. and Niemeyer, Dale and Buchanan, Erin},
  year = {2013},
  month = dec,
  volume = {45},
  pages = {1099--1114},
  issn = {1554-3528},
  journal = {Behavior Research Methods},
  keywords = {Individual differences,Item differences,Large database,Semantic priming},
  language = {en},
  number = {4}
}

@inproceedings{prasadUsingPrimingUncover2019,
  title = {Using {{Priming}} to {{Uncover}} the {{Organization}} of {{Syntactic Representations}} in {{Neural Language Models}}},
  booktitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Prasad, Grusha and {van Schijndel}, Marten and Linzen, Tal},
  year = {2019},
  pages = {66--76},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}}
}

@article{schwanenflugelSemanticRelatednessScope1988,
  title = {Semantic Relatedness and the Scope of Facilitation for Upcoming Words in Sentences},
  author = {Schwanenflugel, Paula J. and LaCount, Kathy L.},
  year = {1988},
  volume = {14},
  pages = {344--354},
  issn = {0278-7393},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  keywords = {college students,Contextual Associations,Lexical Decision,lexical decisions in congruous sentence completion of upcoming words,Semantics,sentence context \& high vs low constraint sentences \& semantic relatedness of unexpected words,Sentence Structure,Word Associations},
  number = {2}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@inproceedings{wang2019superglue,
  title={{Superglue: A stickier benchmark for general-purpose language understanding systems}},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={{Advances in Neural Information Processing Systems}},
  pages={3266--3280},
  year={2019}
}

@inproceedings{wang2018glue,
  title={{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={{International Conference on Learning Representations}},
  year={2018}
}

@inproceedings{levy-etal-2015-supervised,
    title = "Do Supervised Distributional Methods Really Learn Lexical Inference Relations?",
    author = "Levy, Omer  and
      Remus, Steffen  and
      Biemann, Chris  and
      Dagan, Ido",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N15-1098",
    doi = "10.3115/v1/N15-1098",
    pages = "970--976",
}

@inproceedings{gururangan-etal-2018-annotation,
    title = "Annotation Artifacts in Natural Language Inference Data",
    author = "Gururangan, Suchin  and
      Swayamdipta, Swabha  and
      Levy, Omer  and
      Schwartz, Roy  and
      Bowman, Samuel  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2017",
    doi = "10.18653/v1/N18-2017",
    pages = "107--112",
    abstract = "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67{\%} of SNLI (Bowman et. al, 2015) and 53{\%} of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.",
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group},
  url={https://www.nature.com/articles/s42256-020-00257-z}
}

@inproceedings{kaushik-lipton-2018-much,
    title = "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
    author = "Kaushik, Divyansh  and
      Lipton, Zachary C.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1546",
    doi = "10.18653/v1/D18-1546",
    pages = "5010--5015",
    abstract = "Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50{\%} accuracy, sometimes matching the full model. Interestingly, while CBT provides 20-sentence passages, only the last is needed for accurate prediction. By comparison, SQuAD and CNN appear better-constructed.",
}


@book{eisenstein2019introduction,
  title={Introduction to Natural Language Processing},
  author={Eisenstein, Jacob},
  year={2019},
  publisher={MIT Press}
}

@article{goldberg2017neural,
  title={Neural network methods for natural language processing},
  author={Goldberg, Yoav},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={10},
  number={1},
  pages={1--309},
  year={2017},
  publisher={Morgan \& Claypool Publishers}
}

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{miller1995wordnet,
  title={{WordNet: a lexical database for English}},
  author={Miller, George A},
  journal={{Communications of the ACM}},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kassner-schutze-2020-negated,
    title = "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
    author = {Kassner, Nora  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.698",
    doi = "10.18653/v1/2020.acl-main.698",
    pages = "7811--7818",
    abstract = "Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated ({`}{`}Birds cannot [MASK]{''}) and non-negated ({`}{`}Birds can [MASK]{''}) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add {``}misprimes{''} to cloze questions ({`}{`}Talk? Birds can [MASK]{''}). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
}

@article{stanovichPrimingSentenceContext1983,
  title = {On Priming by a Sentence Context},
  author = {Stanovich, Keith E. and West, Richard F.},
  year = {1983},
  volume = {112},
  pages = {1--36},
  issn = {0096-3445},
  abstract = {Previous investigations of sentence context effects (SCEs) on word-naming time have uncovered a pattern of facilitation dominance. Another finding has been that words that are more difficult to recognize in isolation display larger SCEs than easier words. The present experiments with 384 undergraduates showed SCEs to be robust and eliminated several alternative explanations. Two experiments demonstrated the appropriateness of the neutral condition used to assess facilitation and inhibition. Another showed that SCEs did not depend on the procedure used. It was shown that manipulations that were designed to affect S strategies did not change the pattern of results. In 3 experiments, an interaction between stimulus quality and content condition was obtained. The interaction replicated across 2 forms of stimulus degradation, but only 1 form increased inhibition effects as well as facilitation effects. Other inconsistencies between previous SC experiments in the magnitude of the inhibition effects observed were resolved by showing that an SC produced more inhibition in the lexical decision task than in the naming task. It was demonstrated that the 2 tasks produced different amounts of inhibition when the same stimuli were used. Sentence integration processes that occurred after lexical access appeared to be responsible for some of the inhibition observed in lexical decision tasks. (85 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/home/kanishka/Zotero/storage/J8DDJTQT/Stanovich and West - 1983 - On priming by a sentence context.pdf},
  journal = {Journal of Experimental Psychology: General},
  keywords = {college students,Contextual Associations,effect of sentence context on word recognition,empirical \& theoretical ambiguities in interaction between stimulus factors \& contextual information,Experimentation,Form Perception,Humans,Inhibition (Psychology),literature review,Literature Review,Memory,Models; Psychological,Pattern Recognition; Visual,Reaction Time,Reading,Recognition (Learning),Semantics,Sentence Comprehension,Stimulus Parameters,Theory Formulation,Words (Phonetic Units)},
  number = {1}
}

@inproceedings{finkelstein2001placing,
  title={Placing search in context: The concept revisited},
  author={Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
  booktitle={Proceedings of the 10th international conference on World Wide Web},
  pages={406--414},
  year={2001}
}

@inproceedings{agirre-etal-2009-study,
    title = "A Study on Similarity and Relatedness Using Distributional and {W}ord{N}et-based Approaches",
    author = "Agirre, Eneko  and
      Alfonseca, Enrique  and
      Hall, Keith  and
      Kravalova, Jana  and
      Pa{\c{s}}ca, Marius  and
      Soroa, Aitor",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N09-1003",
    pages = "19--27",
}



@inproceedings{bruni-etal-2012-distributional,
    title = "Distributional Semantics in Technicolor",
    author = "Bruni, Elia  and
      Boleda, Gemma  and
      Baroni, Marco  and
      Tran, Nam-Khanh",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-1015",
    pages = "136--145",
}

@inproceedings{armendariz-etal-2020-cosimlex,
    title = "{C}o{S}im{L}ex: A Resource for Evaluating Graded Word Similarity in Context",
    author = "Armendariz, Carlos Santos  and
      Purver, Matthew  and
      Ul{\v{c}}ar, Matej  and
      Pollak, Senja  and
      Ljube{\v{s}}i{\'c}, Nikola  and
      Granroth-Wilding, Mark",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.720",
    pages = "5878--5886",
    abstract = "State of the art natural language processing tools are built on context-dependent word embeddings, but no direct method for evaluating these representations currently exists. Standard tasks and datasets for intrinsic evaluation of embeddings are based on judgements of similarity, but ignore context; standard tasks for word sense disambiguation take account of context but do not provide continuous measures of meaning similarity. This paper describes an effort to build a new dataset, CoSimLex, intended to fill this gap. Building on the standard pairwise similarity task of SimLex-999, it provides context-dependent similarity measures; covers not only discrete differences in word sense but more subtle, graded changes in meaning; and covers not only a well-resourced language (English) but a number of less-resourced languages. We define the task and evaluation metrics, outline the dataset collection methodology, and describe the status of the dataset so far.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{faruqui-etal-2016-problems,
    title = "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks",
    author = "Faruqui, Manaal  and
      Tsvetkov, Yulia  and
      Rastogi, Pushpendre  and
      Dyer, Chris",
    booktitle = "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP}",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2506",
    doi = "10.18653/v1/W16-2506",
    pages = "30--35",
}

@article{lenci2008distributional,
  title={Distributional semantics in linguistic and cognitive research},
  author={Lenci, Alessandro},
  journal={Italian journal of linguistics},
  volume={20},
  number={1},
  pages={1--31},
  year={2008}
}

@article{tversky1977features,
  title={Features of similarity.},
  author={Tversky, Amos},
  journal={Psychological review},
  volume={84},
  number={4},
  pages={327},
  year={1977},
  publisher={American Psychological Association}
}

@article{hutchison2013semantic,
  title={The semantic priming project},
  author={Hutchison, Keith A and Balota, David A and Neely, James H and Cortese, Michael J and Cohen-Shikora, Emily R and Tse, Chi-Shing and Yap, Melvin J and Bengson, Jesse J and Niemeyer, Dale and Buchanan, Erin},
  journal={Behavior research methods},
  volume={45},
  number={4},
  pages={1099--1114},
  year={2013},
  publisher={Springer}
}

@book{mcnamara2005semantic,
  title={Semantic priming: Perspectives from memory and word recognition},
  author={McNamara, Timothy P},
  year={2005},
  publisher={Psychology Press}
}

@inproceedings{chowdhury2018rnn,
  title={RNN simulations of grammaticality judgments on long-distance dependencies},
  author={Chowdhury, Shammur Absar and Zamparelli, Roberto},
  booktitle={Proceedings of the 27th international conference on computational linguistics},
  pages={133--144},
  year={2018}
}

@inproceedings{marvin2018targeted,
  title={Targeted Syntactic Evaluation of Language Models},
  author={Marvin, Rebecca and Linzen, Tal},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1192--1202},
  year={2018}
}

@inproceedings{nivre2016universal,
  title={Universal dependencies v1: A multilingual treebank collection},
  author={Nivre, Joakim and De Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajic, Jan and Manning, Christopher D and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and others},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={1659--1666},
  year={2016}
}

@article{levy2008expectation,
  title={Expectation-based syntactic comprehension},
  author={Levy, Roger},
  journal={Cognition},
  volume={106},
  number={3},
  pages={1126--1177},
  year={2008},
  publisher={Elsevier}
}